* Deployment
** Building a new EAR file
*** Build from from your Hipchat room
You can start a build right from your chatroom!

Go to the <%= trail_customer_shortname %> Hipchat room named: <%= trail_customer_hipchat_room_name %> or goto Hipchat in your [[https://vizrtcustomers.hipchat.com/chat][browser]].

when you are in the chatroom type:
#+BEGIN_SRC sh
guru: build trunk
#+END_SRC
(As of this writing it was not possible to build from a tag or a branch yet)

The chatroom will report on the progress of your build but if you are impatient you can type:
#+BEGIN_SRC sh
guru: jobs
#+END_SRC

Once the build is done, the URI of the finished EAR file is printed in
the chat room. This is the URI you use for the 'ece deploy' command below.

In future you will also be able to deploy an ear to staging in the chatroom.

*** Build from the command line on your builder
Log on to the build server as the user for the given habitat and run
the build script:

To log onto the builder:
#+BEGIN_SRC sh
$ ssh <%= trail_builder_user %>@<%= trail_builder_host %>
#+END_SRC

To build trunk:
#+BEGIN_SRC sh
$ ece-build
#+END_SRC

To build from a tag in subversion:
#+BEGIN_SRC sh
$ ece-build -t <tagname>
#+END_SRC

To build trunk:
#+BEGIN_SRC sh
$ ece-build -b <branchname>
#+END_SRC

Once the build is done, the URI of the finished EAR file is printed in
the shell. This is the URI you use for the 'ece deploy' command below.

** Deploying a new EAR file
In the examples here, we have a build server on <%= trail_builder_host %>
where we build EARs for the the <%= trail_builder_user %> website.

** Make sure you can roll back
Before deploying a new EAR, take note of the previously deployed EAR
so that you can easily roll back:

#+BEGIN_SRC sh
$ sudo ssh escenic@<%= trail_presentation_host %> ece -i engine1 info
[ece#engine-engine1] Deployment state:
[ece#engine-engine1] |-> Version: <%= trail_builder_user %>-trunk-rev4331-<%= trail_today_date %>_1225
[ece#engine-engine1] |-> EAR used: http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4331-<%= trail_today_date %>_1225.ear
[ece#engine-engine1] |-> MD5 sum: 58638b16efc7f4cebd8d0acb4eecfb40
#+END_SRC

If anything goes wrong in [[Deploying a new EAR file for an ECE
instance]], you can just repeat the deployment steps with the URL of the
previous EAR.

*** Deploying a new EAR file for an ECE instance
All the commands in this section is written to be executed on
<%= trail_control_host %> (but you could of course also first log on to each
of the servers and run the commands locally there).

#+BEGIN_SRC sh
$ sudo ssh escenic@<%= trail_presentation_host %> ece -i engine1 --uri http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date %>_1524.ear deploy
[ece#engine-engine1] Deploying http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date %>_1524.ear on engine1 ...
[ece#engine-engine1] Deploying /var/cache/escenic/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date %>_1524.ear on tomcat ...
[ece#engine-engine1] Deployment white list active, only deploying: <%= trail_webapp_name %> escenic-admin indexer-webservice
[ece#engine-engine1] Deployment state file updated: /var/lib/escenic/engine1.state
#+END_SRC

We can now restart the ECE:

#+BEGIN_SRC sh
$ sudo ssh root@<%= trail_presentation_host %> /etc/init.d/ece restart
[ece#engine-engine1] Stopping the engine1 instance of engine on <%= trail_presentation_host %>...
[ece#engine-engine1] Starting the engine1 instance of engine on <%= trail_presentation_host %>...
[ece#search-search1] Stopping the search1 instance of search on <%= trail_presentation_host %>...
[ece#search-search1] Starting the search1 instance of search on <%= trail_presentation_host %>...
#+END_SRC

To confirm see that the ECE instance came up again, do:

#+BEGIN_SRC sh
$ sudo ssh root@<%= trail_presentation_host %> /etc/init.d/ece status
[ece#engine-engine1] UP 0d 0h 11m 45s
[ece#search-search1] UP 0d 0h 11m 42s
#+END_SRC

*** Deploying a new EAR file for a search instance
The search instance is special in that you have to specify the type
parameter. Apart from this, the command is identical. You'll see that
the search instance has a different deployment white list than that of
the ECE instance above.

#+BEGIN_SRC sh
torstein@control:~$ sudo ssh escenic@<%= trail_presentation_host %> ece
-i search1 -t search --uri http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date %>_1524.ear deploy
[ece#search-search1] Deploying http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date %>_1524.ear on search1 ...
[ece#search-search1] Deploying /var/cache/escenic/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date %>_1524.ear on tomcat ...
[ece#search-search1] Deployment white list active, only deploying: escenic-admin solr indexer-webapp
[ece#search-search1] Deployment state file updated: /var/lib/escenic/search1.state
#+END_SRC


** Updating Publication Resources
This document describes how you update publication resources. Here, we
use content-type as an example.

The content type publication resource is special because it's merged
by WF and you thus must take it from the EAR the build server built
for you. Other publication resources could be taken directly from the
SCM, but for consistency, you can just follow the guide below for
these as well (layout, layout-group, menu, community/security and so
on).

First, make sure you have already done the deployment on the
<%= trail_presentation_host %> server. Login to one of the presentation
servers. From <%= trail_control_host %>, do:
#+BEGIN_SRC sh
$ sudo ssh root@<%= trail_presentation_host %>
#+END_SRC

Then, change to the user running the ECE user
#+BEGIN_SRC sh
# su - escenic
#+END_SRC

After this, download current content-type so that we can keep a backup
of it.
#+BEGIN_SRC sh
$ mkdir ~/backup
$ wget -O ~/backup/content-type-$(date --iso)-before-deployment \
       http://localhost:8080/escenic-admin/publication-resources/<%= trail_publication_name %>/escenic/content-type
#+END_SRC

The next step is to locate the new content-type definition.  Go to the
new new content-type from your latest deployment

#+BEGIN_SRC sh
$ cd /opt/tomcat-engine1/webapps/<%= trail_webapp_name %>/META-INF/escenic/publication-resources/escenic/
#+END_SRC

Then, confirm the changes using diff:
#+BEGIN_SRC sh
$ diff -w ~/backup/content-type-$(date --iso)-before-deployment content-type
#+END_SRC

Now, upload the new content-type to ECE.  The command below assumes
that there is a file called the same as the publication resource in
the current directory.
#+BEGIN_SRC sh
$ ece -i engine1 -p <%= trail_publication_name %> -r content-type update
#+END_SRC

To confirm our changes, we now download the resource again to check if
all the changes are the way we want them:
#+BEGIN_SRC sh
$ wget -O ~/backup/content-type-$(date --iso)-after-deployment \
       http://localhost:8080/escenic-admin/publication-resources/<%= trail_publication_name %>/escenic/content-type
#+END_SRC

Finally, run diff again to see that the changes are correct:
#+BEGIN_SRC sh
$ diff -w ~/backup/content-type-$(date --iso)-before-deployment \
          ~/backup/content-type-$(date --iso)-after-deployment
#+END_SRC


** Updating Server Configuration
*** Make changes to the =server-admin= tree
In the <%= trail_builder_user %> source tree, there is a directory
called =server-admin=. This contains all the files that are hand
crafted because the file values cannot be generated by simply running
=ece-install= with the correct parameters.

The structure is as follows: =server-admin/<common|<machine>>/<full
file path>=. Below are some examples to help illustrate how to use
this file tree:

#+BEGIN_SRC text
(1) server-admin/common
(2) server-admin/common/etc/hosts.d
(3) server-admin/<%= trail_presentation_host %>/etc/escenic/ece-engine1.conf
(4) server-admin/<%= trail_db_master_host %>/etc/mysql/my.cnf
#+END_SRC
|------+-----------------------------------------------------------------------------------------|
| Path | Description                                                                             |
|------+-----------------------------------------------------------------------------------------|
| (1)  | Common files for all machines.                                                          |
| (2)  | Files that together generate the =/etc/hosts= when you [[Build New Configuration Packages]] |
| (3)  | The =/etc/escenic/ece-engine1.conf= specific for <%= trail_presentation_host %>         |
| (4)  | The =/etc/mysql/my.cnf= specificf for the <%= trail_db_master_host %> machine.          |
|------+-----------------------------------------------------------------------------------------|

There will always be _some_ files in your =server-admin= tree, but as
a rule of thumb, try to keep this to a minimum.

=ece-install= (and the OS package of course) should provide sensible
defaults for most components given that you pass it the appropriate
settings in the machine's =ece-install.conf=, so ultimately, you'd
only have to check in the =ece-install.conf= for the
<%= trail_control_host %> machine so that it's able to install the
other machines, plus the appropriate file(s) in
=server-admin/common/etc/hosts.d=.

Let's say we want to change the memory setting in =ece-engine1.conf=
for the =<%= trail_presentation_host %>= machine only. Go to your
checked out <%= trail_builder_user %> source code and edit the file
(or indeed add it if it's not already there, in which case would mean
that you're running with the defaults set up by =ece-install=):

#+BEGIN_SRC text
my-machine $ vi ~/src/<%= trail_builder_user %>/server-admin/<%= trail_presentation_host %>/etc/escenic/ece-engine1.conf
#+END_SRC

Make your changes and then commit them using an appropriate ticked ID
in the log message, e.g.:
#+BEGIN_SRC text
my-machine $ svn ci ~/src/<%= trail_builder_user %>/server-admin/<%= trail_presentation_host %>/etc/escenic/ece-engine1.conf -m "<%= trail_builder_user %>-344: increased the max and min heap sizes to 4GB because we've got so many objects"
#+END_SRC

That's it, you're now ready to [[Build New Configuration Packages]]!

*** Build New Configuration Packages
Log on to the build server, just as you did in [[Building a new EAR file]]
When you've issued a build, you'll see that builder also have created
packages for all of the machines you've defined in your =server-admin=
directory tree.

#+BEGIN_SRC text
<%= trail_builder_user %>@<%= trail_builder_host %>:~$ ./build.sh release
[build.sh-0] Starting building @ <%= trail_today_date_full %>
[build.sh-256] Adding an assembly descriptor for Dashboard ...
[build.sh-374] Build SUCCESSFUL! @ <%= trail_today_date_full %>
[build.sh-374] You'll find the release here: http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date_full %>_1524.ear
[build.sh-407] Conf packages available: http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/vosa-conf-<%= trail_presentation_host %>-1-<%= trail_builder_user %>-trunk-r4899.deb
[build.sh-407] Replace '<%= trail_presentation_host %>' with any of: [ <%= trail_editor_host %> <%= trail_db_master_host %> <%= trail_db_slave_host %> <%= trail_import_host %> <%= trail_analysis_host %> <%= trail_presentation_host_list %> ] for the other machines' conf packages.
<%= trail_builder_user %>@<%= trail_builder_host %>:~$
#+END_SRC

*** Deploying the Configuration Packages
Log on to the different hosts, download their DEB package and install
it using =dpkg=. Here, we use <%= trail_presentation_host %> as an
example:

#+BEGIN_SRC text
<%= trail_control_host %>$ ssh <%= trail_presentation_host %>
<%= trail_presentation_host %>$ cd /tmp
<%= trail_presentation_host %>$ wget \
  --quiet \
  --http-user <%= trail_builder_http_user %> \
  --http-password <%= trail_builder_http_password %> \
  http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/vosa-conf-<%= trail_presentation_host %>-1-<%= trail_builder_user %>-trunk-r4899.deb
<%= trail_presentation_host %>$ sudo -i vosa-conf-<%= trail_presentation_host %>-1-<%= trail_builder_user %>-trunk-r4899.deb
#+END_SRC

Because of the mighty =dpkg= and the =DEB= package format, you'll get
prompted for any abnormalities, like if someone has changed any of the
conf package files locally since you last updated the package, if
you've got other, conflicting configuration packages installed on so
on.

Now, you have full control over your configuration being in sync with your EAR deployment. You can easily confirm that you're running the configuration corresponding to your EAR by these two commands:
#+BEGIN_SRC text
<%= trail_presentation_host %>$ -l vosa-conf-<%= trail_presentation_host %> | grep ^ii
ii   vosa-conf-<%= trail_presentation_host %>   1-<%= trail_builder_user%>-trunk-r4899    Server configuration for <%= trail_presentation_host %>
#+END_SRC
#+BEGIN_SRC text
<%= trail_presentation_host %>$ ece -i engine1 info | grep EAR
[ece#engine-engine1] |-> EAR used: http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4899-<%= trail_today_date %>_1524.ear
#+END_SRC

As you can see, both the EAR and configuration are from revision
=4899= of =trunk=. We can now roll back and forth between the various
EAR & configuration builds with confidence that these two always are
in sync.

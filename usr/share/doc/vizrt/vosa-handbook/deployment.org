* Deployment
** Building a new EAR file
In the examples here, we have a build server on <%= trail_builder_host %>
where we build EARs for the the <%= trail_builder_user %> website.

*** Build from from your Hipchat room
You can start a build right from your chatroom!

Go to the <%= trail_customer_shortname %> Hipchat room named: <%= trail_customer_hipchat_room_name %> or goto Hipchat in your [[https://vizrtcustomers.hipchat.com/chat][browser]].

when you are in the chatroom type:
#+BEGIN_SRC sh
guru: build trunk
#+END_SRC
(As of this writing it was not possible to build from a tag or a branch yet)

The chatroom will report on the progress of your build but if you are impatient you can type:
#+BEGIN_SRC sh
guru: jobs
#+END_SRC

Once the build is done, the URI of the finished EAR file is printed in
the chat room. This is the URI you use for the 'ece deploy' command below.

In future you will also be able to deploy an ear to staging in the chatroom.

*** Build from the command line on your build server
Log on to the build server as the user for the given habitat and run
the build script:

To log onto the builder:
#+BEGIN_SRC sh
$ ssh <%= trail_builder_user %>@<%= trail_builder_host %>
#+END_SRC

To build trunk:
#+BEGIN_SRC sh
$ ece-build
#+END_SRC

To build from a tag in subversion:
#+BEGIN_SRC sh
$ ece-build -t <tagname>
#+END_SRC

To build trunk:
#+BEGIN_SRC sh
$ ece-build -b <branchname>
#+END_SRC

Once the build is done, the URI of the finished EAR file and the
corresponding configuration packages for all the machines in your
environment are printed out in the shell:
#+BEGIN_SRC text
[ece-build-0] Starting release creation! @ <%= trail_today_date %>
[ece-build-0] Additional output can be found in /home/<%= trail_builder_user %>/ece-build.log
[ece-build-477] Generating change log for revision 6307 ...
[ece-build-515] Configuration packages available here:
[ece-build-515] http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/vosa-conf-${HOSTNAME}-1-<%= trail_builder_user %>-trunk-r6307.deb
[ece-build-515] Replace '${HOSTNAME}' with any of: [ <%= trail_presentation_host_list %>
[ece-build-515] <%= trail_editor_host %> <%= trail_staging_editor_host %> ] for the other machines' conf packages.
[ece-build-515] BUILD SUCCESSFUL! @ <%= trail_today_date %>
[ece-build-515] You'll find the release here:
[ece-build-515] http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4331-<%= trail_today_date %>_1225.ear
#+END_SRC

The EAR URI can be used the 'ece deploy' command below, and the conf
package(s) can be used with the =ece-deploy= command, or downloaded
manually.

** Deploying a new EAR file
In the examples here, we have a build server on <%= trail_builder_host %>
where we build EARs for the the <%= trail_builder_user %> website.

*** Preparing to deploy to production
If you have changes to be deployed to production you should follow the next checklist:
1. Find out whitch EAR is running in production today.
2. Ensure that your EAR is running on staging and a smoketest has been performed.
2. Make release notes:
  1. Collect all subversion changes that have made it into the EAR that you want to deploy by reviewing all Subversion commits since the last deploy.
  2. List all the Jira from those Subversion commits.
  3. Ensure that all those Jira issues are tested on staging and closed. If you cannot close one, make a new issue for the resulting work and close it anyway.
3. Copy and paste the list of Jira issues in a support case (by email or otherwise) and make sure that the subject contains the name of the EAR file.
4. Expect the support team to perform the deploy within 24 hours or call the support phonnumber ask ask them to do it earlier.
5. If you are a support engineer yourself ask one of your colleagues to deploy to production and follow the checklist there. You shoud not prepare a deploy and than go ahead and deploy it yourself :-).

*** Deploying to a production system
Deployments to production are only done by operators in the Support group of Vizrt Online in Dhaka or Oslo.
If you are getting ready to depoy to a production system you have to follow the next checklist:
0. Verify that you are not the same person who did the changes to the codes :-)
1. Has the EAR been properly release tested?
1. Has the EAR been properly smoke tested?
2. Do the release notes match the changes made to the codes and do they make sense?
3. Are the changes in the EAR causing you to feel that the service will fail after deploy?
4. Is there a valid rollback scenario after deploying the change?
5. Is someone you trust available to help you if you run into trouble rolling back?
6. Does the site on staging show the differences expected when reading the release notes?
If any of these prerequisites is not in place you should refuse the deploy request and notify the user how they can convince you to perform the deploy.


** Make sure you can roll back
Before deploying a new EAR, take note of the previously deployed EAR
so that you can easily roll back:

#+BEGIN_SRC sh
$ sudo ssh escenic@<%= trail_presentation_host %> ece -i engine1 info
[ece#engine-engine1] Deployment state:
[ece#engine-engine1] |-> Version: <%= trail_builder_user %>-trunk-rev4331-<%= trail_today_date %>_1225
[ece#engine-engine1] |-> EAR used: http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4331-<%= trail_today_date %>_1225.ear
[ece#engine-engine1] |-> MD5 sum: 58638b16efc7f4cebd8d0acb4eecfb40
#+END_SRC

If anything goes wrong in [[Deploying%20a%20new%20EAR%20file%20for%20an%20ECE%0Ainstance][Deploying a new EAR file for an ECE
instance]], you can just repeat the deployment steps with the URL of the
previous EAR.

** Deploying a new EAR file for an ECE instance
All the commands in this section is written to be executed on
<%= trail_control_host %> (but you could of course also first log on to each
of the servers and run the commands locally there).

#+BEGIN_SRC sh
$ sudo ssh escenic@<%= trail_presentation_host %> ece -i engine1 --uri http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date %>_1524.ear deploy
[ece#engine-engine1] Deploying http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date %>_1524.ear on engine1 ...
[ece#engine-engine1] Deploying /var/cache/escenic/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date %>_1524.ear on tomcat ...
[ece#engine-engine1] Deployment white list active, only deploying: <%= trail_webapp_name %> escenic-admin indexer-webservice
[ece#engine-engine1] Deployment state file updated: /var/lib/escenic/engine1.state
#+END_SRC

We can now restart the ECE:

#+BEGIN_SRC sh
$ sudo ssh root@<%= trail_presentation_host %> /etc/init.d/ece restart
[ece#engine-engine1] Stopping the engine1 instance of engine on <%= trail_presentation_host %>...
[ece#engine-engine1] Starting the engine1 instance of engine on <%= trail_presentation_host %>...
[ece#search-search1] Stopping the search1 instance of search on <%= trail_presentation_host %>...
[ece#search-search1] Starting the search1 instance of search on <%= trail_presentation_host %>...
#+END_SRC

To confirm see that the ECE instance came up again, do:

#+BEGIN_SRC sh
$ sudo ssh root@<%= trail_presentation_host %> /etc/init.d/ece status
[ece#engine-engine1] UP 0d 0h 11m 45s
[ece#search-search1] UP 0d 0h 11m 42s
#+END_SRC

** Deploying a new EAR file for a search instance
The search instance is special in that you have to specify the type
parameter. Apart from this, the command is identical. You'll see that
the search instance has a different deployment white list than that of
the ECE instance above.

#+BEGIN_SRC sh
torstein@control:~$ sudo ssh escenic@<%= trail_presentation_host %> ece
-i search1 -t search --uri http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date %>_1524.ear deploy
[ece#search-search1] Deploying http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date %>_1524.ear on search1 ...
[ece#search-search1] Deploying /var/cache/escenic/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date %>_1524.ear on tomcat ...
[ece#search-search1] Deployment white list active, only deploying: escenic-admin solr indexer-webapp
[ece#search-search1] Deployment state file updated: /var/lib/escenic/search1.state
#+END_SRC


** Updating Publication Resources
Updating the publication resources is only a matter of passing the
=--update-publication-resources= to the =ece-deploy= command:
#+BEGIN_SRC sh
$ sudo ece-deploy \
   --ear http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date %>_1524.ear \
   --update-publication-resources
#+END_SRC

The list of publications will be pull down from the =/escenic-admin=
web application mounted on all Content Engines running on the host
you're logged into and the publication resources will be located on
the file system under the app server deployment path.

** Updating Server Configuration
*** Make changes to the =server-admin= tree
In the <%= trail_builder_user %> source tree, there is a directory
called =server-admin=. This contains all the files that are hand
crafted because the file values cannot be generated by simply running
=ece-install= with the correct parameters.

The structure is as follows: =server-admin/<common|<machine>>/<full
file path>=. Below are some examples to help illustrate how to use
this file tree:

#+BEGIN_SRC text
(1) server-admin/common
(2) server-admin/common/etc/hosts.d
(3) server-admin/<%= trail_presentation_host %>/etc/escenic/ece-engine1.conf
(4) server-admin/<%= trail_db_master_host %>/etc/mysql/my.cnf
#+END_SRC
|------+-----------------------------------------------------------------------------------------|
| Path | Description                                                                             |
|------+-----------------------------------------------------------------------------------------|
| (1)  | Common files for all machines.                                                          |
| (2)  | Files that together generate the =/etc/hosts= when you [[Build New Configuration Packages]] |
| (3)  | The =/etc/escenic/ece-engine1.conf= specific for <%= trail_presentation_host %>         |
| (4)  | The =/etc/mysql/my.cnf= specificf for the <%= trail_db_master_host %> machine.          |
|------+-----------------------------------------------------------------------------------------|

There will always be _some_ files in your =server-admin= tree, but as
a rule of thumb, try to keep this to a minimum.

=ece-install= (and the OS package of course) should provide sensible
defaults for most components given that you pass it the appropriate
settings in the machine's =ece-install.conf=, so ultimately, you'd
only have to check in the =ece-install.conf= for the
<%= trail_control_host %> machine so that it's able to install the
other machines, plus the appropriate file(s) in
=server-admin/common/etc/hosts.d=.

Let's say we want to change the memory setting in =ece-engine1.conf=
for the =<%= trail_presentation_host %>= machine only. Go to your
checked out <%= trail_builder_user %> source code and edit the file
(or indeed add it if it's not already there, in which case would mean
that you're running with the defaults set up by =ece-install=):

#+BEGIN_SRC text
my-machine $ vi ~/src/<%= trail_builder_user %>/server-admin/<%= trail_presentation_host %>/etc/escenic/ece-engine1.conf
#+END_SRC

Make your changes and then commit them using an appropriate ticked ID
in the log message, e.g.:
#+BEGIN_SRC text
my-machine $ svn ci ~/src/<%= trail_builder_user %>/server-admin/<%= trail_presentation_host %>/etc/escenic/ece-engine1.conf -m "<%= trail_builder_user %>-344: increased the max and min heap sizes to 4GB because we've got so many objects"
#+END_SRC

That's it, you're now ready to [[Build New Configuration Packages]]!

*** Build New Configuration Packages
Log on to the build server, just as you did in [[Building a new EAR file]]
When you've issued a build, you'll see that builder also have created
packages for all of the machines you've defined in your =server-admin=
directory tree.

#+BEGIN_SRC text
<%= trail_builder_user %>@<%= trail_builder_host %>:~$ ece-build
[ece-build-0] Starting building @ <%= trail_today_date_full %>
[ece-build-256] Adding an assembly descriptor for Dashboard ...
[ece-build-374] Build SUCCESSFUL! @ <%= trail_today_date_full %>
[ece-build-374] You'll find the release here: http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date_full %>_1524.ear
[ece-build-407] Conf packages available: http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/vosa-conf-<%= trail_presentation_host %>-1-<%= trail_builder_user %>-trunk-r4899.deb
[ece-build-407] Replace '<%= trail_presentation_host %>' with any of: [ <%= trail_editor_host %> <%= trail_db_master_host %> <%= trail_db_slave_host %> <%= trail_import_host %> <%= trail_analysis_host %> <%= trail_presentation_host_list %> ] for the other machines' conf packages.
<%= trail_builder_user %>@<%= trail_builder_host %>:~$
#+END_SRC

*** Deploying the Configuration Packages
Log on to the different hosts and call =ece-deploy= with the =--conf=
parameter to install the package (you normally do this together with
the EAR file, but for the sake of the example, you /can/ just deploy
the conf package): Here, we use <%= trail_presentation_host %> as an
example:

#+BEGIN_SRC text
<%= trail_control_host %>$ ssh <%= trail_presentation_host %>
<%= trail_presentation_host %>$ sudo ece-insatll --conf \
  http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/vosa-conf-<%= trail_presentation_host %>-1-<%= trail_builder_user %>-trunk-r4899.deb
#+END_SRC

Because of the mighty =dpkg= and the =DEB= package format, you'll get
prompted for any abnormalities, like if someone has changed any of the
conf package files locally since you last updated the package, if
you've got other, conflicting configuration packages installed on so
on.

Now, you have full control over your configuration being in sync with your EAR deployment. You can easily confirm that you're running the configuration corresponding to your EAR by these two commands:
#+BEGIN_SRC text
<%= trail_presentation_host %>$ -l vosa-conf-<%= trail_presentation_host %> | grep ^ii
ii   vosa-conf-<%= trail_presentation_host %>   1-<%= trail_builder_user%>-trunk-r4899    Server configuration for <%= trail_presentation_host %>
#+END_SRC
#+BEGIN_SRC text
<%= trail_presentation_host %>$ ece -i engine1 info | grep EAR
[ece#engine-engine1] |-> EAR used: http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4899-<%= trail_today_date %>_1524.ear
#+END_SRC

As you can see, both the EAR and configuration are from revision
=4899= of =trunk=. We can now roll back and forth between the various
EAR & configuration builds with confidence that these two always are
in sync.

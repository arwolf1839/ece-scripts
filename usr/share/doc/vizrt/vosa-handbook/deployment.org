* Deployment
** Building a new EAR file and configuration packages
For a full deployment, two artifacts are needed: an EAR file and a
configuration package for each of the machines. The build server
provides you with both.

*** Build from from your Hipchat room
You can start a build right from your chatroom! Go to the <%= trail_customer_shortname %> Hipchat room named: <%= trail_customer_hipchat_room_name %> or go to Hipchat in your [[https://vizrtcustomers.hipchat.com/chat][browser]].

When you are in the chatroom type:
#+BEGIN_SRC sh
guru: build trunk
#+END_SRC
(As of this writing it was not possible to build from a tag or a branch yet)

The chatroom will report on the progress of your build but if you are impatient you can type:
#+BEGIN_SRC sh
guru: jobs
#+END_SRC

Once the build is done, the URI of the finished EAR file is printed in
the chat room. This is the URI you use for the =ece deploy= command below.

In future you will also be able to deploy an EAR to staging in the chatroom.

*** Build from the command line on your build server
Log on to the build server as the user for the given habitat and run
the build script:

#+BEGIN_SRC sh
$ ssh <%= trail_builder_user %>@<%= trail_builder_host %>
$ ece-build
#+END_SRC
This will build from trunk. 

If you wish to build from a tag in Subversion you do:
#+BEGIN_SRC sh
$ ece-build -t <tagname>
#+END_SRC

And to build a branch you can use the =-b= parameter instead:
#+BEGIN_SRC sh
$ ece-build -b <branchname>
#+END_SRC

Once the build is done, the URI of the finished EAR file and the
corresponding configuration packages for all the machines in your
environment are printed out in the shell:
#+BEGIN_SRC text
[ece-build-0] Starting release creation! @ <%= trail_today_date %>
[ece-build-0] Additional output can be found in /home/<%= trail_builder_user %>/ece-build.log
[ece-build-477] Generating change log for revision 6307 ...
[ece-build-515] Configuration packages available here:
[ece-build-515] http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/vosa-conf-${HOSTNAME}-1-<%= trail_builder_user %>-trunk-r6307.deb
[ece-build-515] Replace '${HOSTNAME}' with any of: [ <%= trail_presentation_host_list %>
[ece-build-515] <%= trail_editor_host %> <%= trail_staging_editor_host %> ] for the other machines' conf packages.
[ece-build-515] BUILD SUCCESSFUL! @ <%= trail_today_date %>
[ece-build-515] You'll find the release here:
[ece-build-515] http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev6307-<%= trail_today_date %>_1225.ear
#+END_SRC

The EAR and configuration package URIs can be used the =ece deploy= command below.

** Deploying a new EAR file
In the examples here, we have a build server on <%= trail_builder_host %>
where we build EARs for the the <%= trail_builder_user %> website.

*** Preparing to deploy to production

**** Get the current version of the EAR and conf package
First, we need to know which is the currently deployed EAR (to see all the deployments
remove the pipe to =tail -1=):
#+BEGIN_SRC text
$ ece -i engine1 list-deployments | tail -1
Thu Dec 13 15:05:41 IST 2012 <%= trail_customer_acronym %>-trunk-rev6260-2012-12-12_1401.ear c6c762523db66ae21cdf7f7f00016f7f
#+END_SRC

Also, confirm that the =vosa-conf= configruation package correspond to
the EAR version:
#+BEGIN_SRC text
$ dpkg -l vosa-conf-<%= trail_presentation_host %> | grep ^ii
ii  vosa-conf-<%= trail_presentation_host %> 1-<%=trail_customer_acronym %>-trunk-r6260  Server configuration for <%= trail_presentation_host %>
#+END_SRC 
If the version of the EAR and vosa-conf package differ, you must find out why (ask your colleagues) or if there are any notable changes that has been skipped (this is normally the case, that someone has updated the EAR without updating the conf package. However, this *should* never happen if everyone always uses =ece-deploy=.

**** Read through the change log and code diff
Second, read through the change log and code diff for the new EAR
and conf packages. These are present as one =.report= and one =.diff=
file for each build that the builder makes for your project. If you're
working on trunk, you'll find all these under http://<%= trail_builder_host %>/<%= trail_builder_user %>/changelogs/trunk. 
Replace =trunk= with =branches/<branch>= if you're working on a branch.

You will see a number of files e.g.:
#+BEGIN_SRC text
from-6100-to-6200.report
from-6100-to-6200.diff
from-6200-to-6250.report
from-6200-to-6250.diff
from-6250-to-6255.report
from-6250-to-6255.diff
#+END_SRC

Now, if the new EAR you built in [[Build new EAR]] has revision =6255= and
the current deployed EAR has revision =6200=, you must look at the
files which contains the changes in between, namely:
=from-6200-to-6250.report=, 
=from-6200-to-6250.diff=,
=from-6250-to-6255.report= and =from-6250-to-6255.diff=.

The report files contain a generated summary of the related JIRA
issues that have been worked on with this build, as well as a
*risk assesment scrore*. This score is calculated from the code diffs and
diff contexts.

**** Deploy & test on staging
Log on to <%= trail_staging_editor_host %> and make the deployment:
#+BEGIN_SRC text
$ ssh <%= trail_staging_editor_host %>
$ sudo ece-deploy \
  --ear  http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev6307-<%= trail_today_date %>_1225.ear \
  --conf http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/vosa-conf-${HOSTNAME}-1-<%= trail_builder_user %>-trunk-r6307.deb \
  --update-publication-resources
#+END_SRC

This will deploy the new configuration, update all the publication
resources of all your publications and update all ECEs, search
instances and EAEs you have on your machine. =ece-deploy= looks in
=/etc/default/ece= to determine which instances to deploy to, just
like how =/etc/init.d/ece= decides which instances to start and stop.

Ensure that your ECE, Search and EAE instances are running:
#+BEGIN_SRC text
$ sudo /etc/init.d/ece status
[ece#engine-engine1] UP 0d 0h 1m 53s
[ece#search-search1] UP 0d 0h 1m 53s
[ece#analysis-analysis1] UP 0d 0h 1m 53s
#+END_SRC

Perform a smoke test. The command below will call the local ECE with
=Host= header set and output the amount of bytes returned. If this
number is less than a few thousand, you should immediately investigate
why. Also, we check that there's a =<title/>= element returned from
the front page of each of the domains:
#+BEGIN_SRC text
$ for host in <%= trail_virtual_host_list %>; do \
    echo "${host}'s title:" 
    curl --silent --header "Host: $host" http://localhost:8080/ | grep -A 1 '<title>'; \
    echo "${host}'s front page bytes:"; \
    curl --silent --header "Host: $host" http://localhost:8080/ | wc -c; \
  done
#+END_SRC

**** Confirm that all related JIRA issues are fixed
Ensure that all the listed JIRA issues in the report are tested on <%= trail_uat_url %> and closed. If you cannot close one, make a new issue for the
resulting work and close it anyway.

**** Update support cases
Copy and paste the list of Jira issues in a support case (by email or
otherwise) and make sure that the subject contains the name of the EAR
file.

*** Deploying to a production system
Deployments to production are only done by operators in the Support group of Vizrt Online in Dhaka or Oslo.
If you are getting ready to deploy to a production system you have to follow the next checklist:
0. Verify that you are not the same person who did the changes to the code :-)
1. Has the EAR & configuration package been properly _release_ tested?
1. Has the EAR & configuration package been properly _smoke_ tested?
2. Do the release notes match the changes made to the code and do they make sense?
3. Are the changes in the EAR causing you to feel that the service will fail after deploy?
5. Is someone you trust available to help you if you run into trouble rolling back?
6. Does the site on staging show the differences expected when reading the release notes?

If any of these prerequisites is not in place you should refuse the deploy request and notify the user how they can convince you to perform the deploy.

If, on the other hand, you can answer yes to all of the above, you can
go ahead and deploy on production. The steps are pretty much the same
as described in [[Deploy & test on staging]], with the exception that you
also must:

1. First, [[Schedule downtime]] of each machine you Update
2. If the machine you're updating, you must remove it from the load
   balancer (<%= trail_lb_host %>) that receives the incoming web
   traffic.
3. Log on to the machine and use =ece-deploy= to deploy the new EAR
   and configuration package.
4. Remove the scheduled downtime of the machine from
   http://<%= trail_monitoring_host %>/icinga

** Schedule downtime
You can either schedule down time of a particular machine by using the
web interface at http://<%= trail_monitoring_host %>/icinga or by
logging on to <%= trail_control_host %> and use the command
=downtime=:

#+BEGIN_SRC sh
$ ssh <%= trail_control_host %>
$ echo "Upgrading <%= trail_presentation_host %> to fix caching problem" | \
  downtime -i <%= trail_presentation_host %> 1 hours
#+END_SRC

** Rolling back to a previous version
You can roll back to any previous deployment you've done using
=ece-deploy=. To get a list of all previous deployments done with
=ece-deploy=, you do:
#+BEGIN_SRC sh
$ ssh <%= trail_presentation_host %>
$ sudo ece-deploy --list-deployments
   - Deployment <%= trail_presentation_host %>-1355390454 was made @ Thu Dec 13 14:50:54 IST 2012
   - Deployment <%= trail_presentation_host %>-1355320868 was made @ Wed Dec 12 19:31:08 IST 2012
   - Deployment <%= trail_presentation_host %>-1355319440 was made @ Wed Dec 12 19:07:20 IST 2012
   - Deployment <%= trail_presentation_host %>-1354621048 was made @ Tue Dec 4 17:07:28 IST 2012
   - Deployment <%= trail_presentation_host %>-1354540403 was made @ Mon Dec 3 18:43:23 IST 2012
#+END_SRC

Normally, the previous one will be the right one to roll back to, but
if you've played a lot back and forth If you don't know which one to
choose, then pick the one that's fairly recent and has been running
for a long time, i.e., there's a long span between that deployment and
the next one. 

From the output above, we see that the one from the 4th of December
has been running the longest, so we roll back to that one with a
simple command:
#+BEGIN_SRC sh
$ sudo ece-deploy --rollback <%= trail_presentation_host %>-1354621048 --update-publication-resources
#+END_SRC

The reason why =ece-deploy= has its own deployment ID and doesn't use
the version of the EAR & configuration package, is that it's possible
to make several deployments of the same EAR/configuration package,
even on the same host. Furthermore, =ece-deploy= deploys on several
instances, not only one. And lastly, it's even possible to choose
whether or not to update the publication resources. Hence,
=ece-deploy= has its own IDs and database of its deployments to make
everything reproduce-able.

In this connection, it should also be noted that each of the ECE
instances also have their own [[Instance deployment log]]

** Manually deploying a new EAR file to an ECE instance
We strongly recommend that you use =ece-deploy= to deploy a new EAR
file. If you only want to deploy the EAR and not the configuration
package, you can just call =ece-deploy= without the =--conf=
parameter.

However, if you for some reason, perhaps you don't have root privileges on the machine, and want to deploy an EAR to a specific instance, you can use =ece deploy= (note that =ece-deploy= is different from =ece deploy=):

#+BEGIN_SRC sh
$ ece -i engine1 \
      --uri http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date %>_1524.ear \
      deploy \
      restart
#+END_SRC

You can confirm that the instance came up again by querying =ece -i
engine1 info= and looking in the [[Instance deployment log]] to see that
the new EAR has been deployed.

** Manually deploying a new EAR file to a search instance
Again, we recommend you using =ece-deploy= for this, but if you really
want to do it explicitly for a search instance, this is the same as
[[Manually deploying a new EAR file to an ECE instance]] except that you
must add =-type search= to the =ece= command:

#+BEGIN_SRC sh
$ ece -i search1 \
      -t search \
      --uri http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date %>_1524.ear \
      deploy \
      restart
#+END_SRC

** Instance deployment log
Each of the ECE, EAE and search instances also have their own
deployment log where the EAR used whenever running
=ece -i <%= trail_presentation_host %> deploy= along with its MD5 sum and the date
of deployment is available:
#+BEGIN_SRC sh
$ ece -i engine1 list-deployments 
[ece#engine-engine1] These are all the deployments made on engine1:
Wed Dec 12 19:11:39 IST 2012 nie-trunk-rev6259-2012-10-12_1322.ear c6c7643234asdfasdfdf7f7f0001612e
Wed Dec 12 19:31:35 IST 2012 nie-trunk-rev6260-2012-12-12_1401.ear c6c762523db66ae21cdf7f7f00016f7f
#+END_SRC
This log file is automatically updated when you use the =ece-deploy= command.


** Updating Server Configuration
*** Make changes to the =server-admin= tree
In the <%= trail_builder_user %> source tree, there is a directory
called =server-admin=. This contains all the files that are hand
crafted because the file values cannot be generated by simply running
=ece-install= with the correct parameters.

The structure is as follows: =server-admin/<common|<machine>>/<full
file path>=. Below are some examples to help illustrate how to use
this file tree:

#+BEGIN_SRC text
(1) server-admin/common
(2) server-admin/common/etc/hosts.d
(3) server-admin/<%= trail_presentation_host %>/etc/escenic/ece-engine1.conf
(4) server-admin/<%= trail_db_master_host %>/etc/mysql/my.cnf
#+END_SRC
|------+-----------------------------------------------------------------------------------------|
| Path | Description                                                                             |
|------+-----------------------------------------------------------------------------------------|
| (1)  | Common files for all machines.                                                          |
| (2)  | Files that together generate the =/etc/hosts= when you [[Build New Configuration Packages]] |
| (3)  | The =/etc/escenic/ece-engine1.conf= specific for <%= trail_presentation_host %>         |
| (4)  | The =/etc/mysql/my.cnf= specificf for the <%= trail_db_master_host %> machine.          |
|------+-----------------------------------------------------------------------------------------|

There will always be _some_ files in your =server-admin= tree, but as
a rule of thumb, try to keep this to a minimum.

=ece-install= (and the OS package of course) should provide sensible
defaults for most components given that you pass it the appropriate
settings in the machine's =ece-install.conf=, so ultimately, you'd
only have to check in the =ece-install.conf= for the
<%= trail_control_host %> machine so that it's able to install the
other machines, plus the appropriate file(s) in
=server-admin/common/etc/hosts.d=.

Let's say we want to change the memory setting in =ece-engine1.conf=
for the =<%= trail_presentation_host %>= machine only. Go to your
checked out <%= trail_builder_user %> source code and edit the file
(or indeed add it if it's not already there, in which case would mean
that you're running with the defaults set up by =ece-install=):

#+BEGIN_SRC text
$ vi ~/src/<%= trail_builder_user %>/server-admin/<%= trail_presentation_host %>/etc/escenic/ece-engine1.conf
#+END_SRC

Make your changes and then commit them using an appropriate ticked ID
in the log message, e.g.:
#+BEGIN_SRC sh
$ svn ci ~/src/<%= trail_builder_user %>/server-admin/<%= trail_presentation_host %>/etc/escenic/ece-engine1.conf \
      -m "<%= trail_builder_user %>-344: increased the max and min heap sizes to 4GB because we've got so many objects"
#+END_SRC

That's it, you're now ready to [[Build New Configuration Packages]]!

*** Build New Configuration Packages
Log on to the build server, just as you did in [[Building a new EAR file]]
When you've issued a build, you'll see that builder also have created
packages for all of the machines you've defined in your =server-admin=
directory tree:

#+BEGIN_SRC text
<%= trail_builder_user %>@<%= trail_builder_host %>:~$ ece-build
[ece-build-0] Starting building @ <%= trail_today_date_full %>
[ece-build-256] Adding an assembly descriptor for Dashboard ...
[ece-build-374] Build SUCCESSFUL! @ <%= trail_today_date_full %>
[ece-build-374] You'll find the release here: http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4121-<%= trail_today_date_full %>_1524.ear
[ece-build-407] Conf packages available: http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/vosa-conf-<%= trail_presentation_host %>-1-<%= trail_builder_user %>-trunk-r4899.deb
[ece-build-407] Replace '<%= trail_presentation_host %>' with any of: [ <%= trail_editor_host %> <%= trail_db_master_host %> <%= trail_db_slave_host %> <%= trail_import_host %> <%= trail_analysis_host %> <%= trail_presentation_host_list %> ] for the other machines' conf packages.
<%= trail_builder_user %>@<%= trail_builder_host %>:~$
#+END_SRC

*** Deploying the Configuration Packages
Log on to the different hosts and call =ece-deploy= with the =--conf=
parameter to install the package (you normally do this together with
the EAR file, but for the sake of the example, you /can/ just deploy
the conf package): Here, we use <%= trail_presentation_host %> as an
example:

#+BEGIN_SRC text
<%= trail_control_host %>$ ssh <%= trail_presentation_host %>
<%= trail_presentation_host %>$ sudo ece-deploy --conf \
  http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/vosa-conf-<%= trail_presentation_host %>-1-<%= trail_builder_user %>-trunk-r4899.deb
#+END_SRC

Because of the mighty =dpkg= and the =DEB= package format, you'll get
prompted for any abnormalities, like if someone has changed any of the
conf package files locally since you last updated the package, if
you've got other, conflicting configuration packages installed on so
on.

Now, you have full control over your configuration being in sync with your EAR deployment. You can easily confirm that you're running the configuration corresponding to your EAR by these two commands:
#+BEGIN_SRC text
<%= trail_presentation_host %>$ -l vosa-conf-<%= trail_presentation_host %> | grep ^ii
ii   vosa-conf-<%= trail_presentation_host %>   1-<%= trail_builder_user%>-trunk-r4899    Server configuration for <%= trail_presentation_host %>
#+END_SRC
#+BEGIN_SRC text
<%= trail_presentation_host %>$ ece -i engine1 info | grep EAR
[ece#engine-engine1] |-> EAR used: http://<%= trail_builder_host %>/<%= trail_builder_user %>/releases/<%= trail_builder_user %>-trunk-rev4899-<%= trail_today_date %>_1524.ear
#+END_SRC

As you can see, both the EAR and configuration are from revision
=4899= of =trunk=. We can now roll back and forth between the various
EAR & configuration builds with confidence that these two always are
in sync.
